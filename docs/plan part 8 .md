ğŸ“šâ€¯Ultraâ€‘Detailed Project Roadâ€‘Map
Repository: https://github.com/gainey666/-tetris-overlay-test

Goal: Transform the current prototype into a productionâ€‘ready, fullyâ€‘tested, extensible, and wellâ€‘documented Tetris overlay that:

Runs a stable 30â€¯FPS capture â†’ prediction â†’ ghostâ€‘render loop.
Exposes a polished Qt Settings UI (live preview, hotâ€‘key editor, colour/opacity picker, visualâ€‘flags).
Tracks every frame in an SQLite database and shows a rich Statistics Dashboard (charts, export, filtering).
Provides a robust CI pipeline (unit, integration, UI, lint, typeâ€‘checking, performanceâ€‘benchmarks).
Is fully documented, packaged, and ready for contributors to add new prediction agents or visual features.
Because you asked for â€œa lot more workâ€, the plan below breaks the effort into nine major phases, each containing epics, userâ€‘stories, technical tasks, acceptance criteria, estimated time, and dependencies.

You can hand this roadmap to a team (or a single LLM) and let it work AFK for daysâ€”each block is independent enough to be taken on in parallel, but the ordering guarantees a clean, stable build at any checkpoint.

ğŸ“‘ Table of Contents
Phaseâ€¯0 â€“ Preparations & Baseline
Phaseâ€¯1 â€“ Core Loop Refactor & Realâ€‘Time Guarantees
Phaseâ€¯2 â€“ Fullâ€‘Featured Settings UI
Phaseâ€¯3 â€“ Ghost Rendering Engine (Shapes, Colours, Effects)
Phaseâ€¯4 â€“ Statistics Collection, Storage & Dashboard
Phaseâ€¯5 â€“ Comprehensive Test Suite (Unit, Integration, UI, Performance)
Phaseâ€¯6 â€“ Continuousâ€‘Integration & Release Automation
Phaseâ€¯7 â€“ Documentation, Packaging & Distribution
Phaseâ€¯8 â€“ Performance, Profiling, and Optimization
Phaseâ€¯9 â€“ Futureâ€‘Proofing & Extensibility
Overall Timeline & Milestones
Risks & Mitigations
How to Hand This to an LLM / Automation Script
ğŸš¦ Phaseâ€¯0 â€“ Preparations & Baseline
Epic	Description	Tasks	Estimate
0â€‘A	Audit current repo â€“ generate a clean list of all files, missing imports, and duplicated code.	â€¢ Run git lsâ€‘files and store in repoâ€‘inventory.txt.
â€¢ Run python -m pip install -r requirements.txt locally and capture any import errors.
â€¢ Produce a markdown â€œCurrentâ€‘Stateâ€ report summarizing missing modules, unused files, test coverage, and CI health.	2â€¯h
0â€‘B	Create a dev branch for the entire refactor.	git checkout -b dev/fullâ€‘refactor	5â€¯min
0â€‘C	Add a â€œpreâ€‘commitâ€ hook that runs ruff, black, and mypy locally before each commit.	â€¢ Add .pre-commit-config.yaml with those hooks.
â€¢ Run pre-commit install.	30â€¯min
0â€‘D	Automated baseline testing â€“ capture current test failures.	pytest -q > baseline_tests.txt and commit the file for reference.	15â€¯min
0â€‘E	Define coding standards (PEPâ€‘8, type hints, docâ€‘strings). Create a CONTRIBUTING.md with â€œhow to run testsâ€.	1â€¯h	
Result: A clean development environment, a baseline report, and a branch ready for massive changes.

âš™ï¸ Phaseâ€¯1 â€“ Core Loop Refactor & Realâ€‘Time Guarantees
Why: The current run_overlay_core.py defines process_frames() but never calls it, creates a new OverlayRenderer every tick, and hardâ€‘codes the piece. All of this prevents a stable 30â€¯FPS loop.

Epicâ€¯1â€‘A â€“ Global Renderer & Singleton Settings
Task	Detail	Acceptance
1â€‘Aâ€‘01	Add ui/current_settings.py (singleton exposing CURRENT and an update() helper).	from ui.current_settings import CURRENT works anywhere.
1â€‘Aâ€‘02	Refactor run_overlay_core.py to import CURRENT and create a single OverlayRenderer instance at module load time.	renderer.visible toggles correctly via F9 (no new instance per frame).
1â€‘Aâ€‘03	Ensure OverlayRenderer can be accessed from any module without circular imports (use local imports inside methods).	No ImportError on running the overlay.
Estimated effort: 4â€¯h

Epicâ€¯1â€‘B â€“ Dynamic Hotâ€‘Key System
Task	Detail	Acceptance
1â€‘Bâ€‘01	Remove static _register_hotkeys implementation. Replace with _register_dynamic_hotkeys() that reads CURRENT.hotkeys.	Changing a hotâ€‘key in Settings updates the binding instantly.
1â€‘Bâ€‘02	Add a Qtâ€‘compatible launcher for the Statistics Dashboard (_show_dashboard() helper that checks QApplication.instance()).	Dashboard opens from any hotâ€‘key without â€œQApplication already existsâ€ crash.
1â€‘Bâ€‘03	Register the Settings dialog hotâ€‘key (open_settings) to launch SettingsDialog.	Pressing the configured key opens the Settings UI.
1â€‘Bâ€‘04	Add automatic reâ€‘registration after settings_changed signal.	Updating a hotâ€‘key immediately takes effect (no restart).
Estimated effort: 5â€¯h

Epicâ€¯1â€‘C â€“ Frameâ€‘Worker Thread (30â€¯FPS)
Task	Detail	Acceptance
1â€‘Câ€‘01	Implement _frame_worker() that loops at target_fps = 30 using time.sleep(max(0, interval - elapsed)).	Log shows â€œProcessed frame #Nâ€ ~30 times per second.
1â€‘Câ€‘02	Wrap each iteration in a try/except that logs exception with stack trace but never crashes the thread.	CI runs the worker for 500 frames without a crash.
1â€‘Câ€‘03	Start the worker as daemon before calling run_overlay().	Program exits cleanly on Esc; the thread terminates automatically.
1â€‘Câ€‘04	Add frameâ€‘counter and latency measurement (frame_start_ts).	LOGGER.info includes latency_ms for every frame.
1â€‘Câ€‘05	Add graceful shutdown that stops the worker (by setting a threading.Event) and flushes telemetry logs.	No dangling threads after exit.
Estimated effort: 6â€¯h

Epicâ€¯1â€‘D â€“ Prediction Agent Integration
Task	Detail	Acceptance
1â€‘Dâ€‘01	Replace hardâ€‘coded "T" with piece detection from the nextâ€‘queue (see Phaseâ€¯3).	prediction_agent.handle receives the actual piece and orientation.
1â€‘Dâ€‘02	Add fallback to "mock" agent if detection fails (ensures overlay never stalls).	Overlay continues with a placeholder ghost.
1â€‘Dâ€‘03	Extend the predictionâ€‘agent interface to return additional flags (is_tspin, is_b2b, combo).	draw_ghost can use those flags for visual effects.
Estimated effort: 3â€¯h

ğŸ–¥ï¸ Phaseâ€¯2 â€“ Fullâ€‘Featured Settings UI
Epicâ€¯2â€‘A â€“ Settings Model & Persistence (already present, but solidify)
Task	Detail	Acceptance
2â€‘Aâ€‘01	Validate JSON schema using jsonschema (add settings_schema.json).	load_settings() raises ValidationError on malformed file.
2â€‘Aâ€‘02	Add migration logic for future schema versions (CURRENT_VERSION in file).	Old configs upgrade automatically.
2â€‘Aâ€‘03	Write unit tests for schema validation and migration.	pytest tests/test_settings_schema.py passes.
Estimated effort: 2â€¯h

Epicâ€¯2â€‘B â€“ Qt Settings Dialog Enhancements
Task	Detail	Acceptance
2â€‘Bâ€‘01	Add tabbed layout: General, Ghost, Hotâ€‘keys, Visual Flags, Advanced.	UI shows 5 tabs with clear headings.
2â€‘Bâ€‘02	Live preview widget draws the actual tetromino shape (using PIECE_SHAPES) and updates instantly when colour/opacity changes.	Changing the colour slider updates the preview in <â€¯100â€¯ms.
2â€‘Bâ€‘03	Hotâ€‘key editor â€“ use QKeySequenceEdit for each hotâ€‘key, store the string in lowerâ€‘case.	User can press Ctrl+Shift+G â†’ field shows â€œCtrl+Shift+Gâ€.
2â€‘Bâ€‘04	Resetâ€‘toâ€‘defaults button that restores all fields and writes the defaults to disk.	After reset, CURRENT matches Settings() default values.
2â€‘Bâ€‘05	Validation feedback â€“ UI warns on bad ROI format (nonâ€‘numeric, wrong count).	Invalid ROI entry disables â€œOKâ€ and shows a red tooltip.
2â€‘Bâ€‘06	Persist changes on Apply / OK â€“ emit settings_changed signal.	Closing the dialog updates live overlay without restart.
2â€‘Bâ€‘07	Darkâ€‘mode support (optional) â€“ use Qtâ€™s QPalette to adapt colours.	UI respects system darkâ€‘mode on Windows/macOS.
Estimated effort: 9â€¯h

Epicâ€¯2â€‘C â€“ Settingsâ€‘Driven Visual Flags
Task	Detail	Acceptance
2â€‘Câ€‘01	Extend OverlayRenderer.draw_ghost to read CURRENT.show_combo and CURRENT.show_b2b and render additional graphics (green combo bar, red B2B outline).	Visual cues appear/disappear as the flags are toggled.
2â€‘Câ€‘02	Add runtime toggle in the Settings UI â€œVisual Flagsâ€ tab (checkboxes).	Changing the checkbox updates the overlay instantly (no reâ€‘load).
Estimated effort: 2â€¯h

ğŸ§© Phaseâ€¯3 â€“ Ghost Rendering Engine
Epicâ€¯3â€‘A â€“ Real Tetromino Shapes
Task	Detail	Acceptance
3â€‘Aâ€‘01	Import PIECE_SHAPES from the Dellacherie agent (or create a shared tetromino_shapes.py).	PIECE_SHAPES["T"][0] returns list of (x,y) offsets.
3â€‘Aâ€‘02	Refactor OverlayRenderer.draw_ghost to iterate over the shape cells, apply rotation, and draw each cell with the current colour/opacity.	Ghost matches the exact shape (e.g., â€œLâ€ rotated 2).
3â€‘Aâ€‘03	Add optional â€œoutlineâ€ mode (draw just borders) for better visibility on bright backgrounds.	Configurable via Settings â†’ Ghost â†’ â€œOutline onlyâ€.
3â€‘Aâ€‘04	Unit test for each shape + rotation (compare pixel buffer to expected pattern).	pytest tests/test_ghost_shapes.py passes.
Estimated effort: 5â€¯h

Epicâ€¯3â€‘B â€“ Ghost Visual Effects
Task	Detail	Acceptance
3â€‘Bâ€‘01	Add fadeâ€‘out animation (ghost gradually becomes more transparent as it approaches the ground).	Animation runs at 30â€¯FPS, no stutter.
3â€‘Bâ€‘02	Implement shadow blur using pygame.Surface with pygame.BLEND_RGBA_MULT.	Ghost looks â€œsoftâ€.
3â€‘Bâ€‘03	Provide userâ€‘configurable effect (ghost_effect enum: solid, outline, fade, blur).	Settings UI dropdown updates rendering instantly.
Estimated effort: 4â€¯h

ğŸ“Š Phaseâ€¯4 â€“ Statistics Collection, Storage & Dashboard
Epicâ€¯4â€‘A â€“ Stats Collector Refactor
Task	Detail	Acceptance
4â€‘Aâ€‘01	Replace the adâ€‘hoc stats/collector.py with a service class (StatsService) that tracks match lifecycle, frame counters, and aggregates.	Single class with start_match(), end_match(), record_frame().
4â€‘Aâ€‘02	Add automatic combo/B2B detection inside record_frame() (use previous frameâ€™s data).	Event rows contain correct combo and b2b boolean.
4â€‘Aâ€‘03	Add SQLModel relationships (Match.events backâ€‘ref) for easy querying.	Match.events works in ORM style.
4â€‘Aâ€‘04	Add indexing on match_id + frame for fast dashboard queries.	SQLite EXPLAIN QUERY PLAN shows index usage.
4â€‘Aâ€‘05	Write unit tests for start/end/record logic (including edge cases like missing start).	pytest tests/test_stats_service.py passes.
Estimated effort: 6â€¯h

Epicâ€¯4â€‘B â€“ Pieceâ€‘Detection Integration
Task	Detail	Acceptance
4â€‘Bâ€‘01	Implement a simple colourâ€‘based detector (piece_detector.py) that analyses the first queue image and returns piece + orientation.	Works for the default Tetris skin (â‰¥â€¯90â€¯% accuracy).
4â€‘Bâ€‘02	Add fallback to mock detection if the colour histogram is ambiguous.	No crash when detection fails.
4â€‘Bâ€‘03	Write parameterised tests using synthetic PNGs for each piece (store them in tests/fixtures/pieces/).	pytest verifies each colour range maps to the right piece.
4â€‘Bâ€‘04	Hook the detector into process_frames (replace hardâ€‘coded "T").	Ghost now matches the piece that appears in the queue.
Estimated effort: 5â€¯h

Epicâ€¯4â€‘C â€“ Statistics Dashboard UI
Task	Detail	Acceptance
4â€‘Câ€‘01	Refactor ui/stats_dashboard.py to decouple data loading from UI (use a DashboardModel class).	UI can be refreshed without reâ€‘creating the model.
4â€‘Câ€‘02	Add filter controls: date range picker, agent selector, score threshold.	Users can limit the view to a subset of matches.
4â€‘Câ€‘03	Implement three Matplotlib charts (score over time, combo streak, piece distribution) with toolâ€‘tips (via mplcursors).	Hovering over a point shows the exact value.
4â€‘Câ€‘04	Add export buttons (CSV, JSON) that write only the currently filtered dataset.	Export respects the filter settings.
4â€‘Câ€‘05	Enable darkâ€‘mode (Qt palette + Matplotlib style).	UI respects system darkâ€‘mode and settings toggle.
4â€‘Câ€‘06	Write Qt UI tests (pytestâ€‘qt) for opening the dashboard, applying a filter, and exporting a file.	Tests run in CI without a display (use xvfb).
4â€‘Câ€‘07	Add responsive resizing â€“ charts adapt to window size.	No clipping when the window is resized.
Estimated effort: 10â€¯h

âœ… Phaseâ€¯5 â€“ Comprehensive Test Suite
Epicâ€¯5â€‘A â€“ Unit Tests Expansion
Task	Detail	Acceptance
5â€‘Aâ€‘01	100â€¯% coverage for ui/settings.py, ui/settings_storage.py, ui/current_settings.py.	coverage run -m pytest && coverage report â‰¥â€¯100â€¯% for those modules.
5â€‘Aâ€‘02	Mockâ€‘based tests for run_overlay_core.process_frames (patch DualScreenCapture, capture_shared_ui, next_queue_capture).	Frame processing runs without actual screen capture.
5â€‘Aâ€‘03	Edgeâ€‘case tests for invalid ROI strings, missing hotâ€‘key entries, and corrupted settings.json.	Each raises a clear ValueError with helpful message.
5â€‘Aâ€‘04	Parameterised tests for OverlayRenderer.draw_ghost with every tetromino shape & rotation.	Visualâ€‘pixel buffer matches expected pattern (use pygame.surfarray).
Estimated effort: 8â€¯h

Epicâ€¯5â€‘B â€“ Integration Tests (Full Stack)
Task	Detail	Acceptance
5â€‘Bâ€‘01	Spawn the whole application in a headless Xvfb session, trigger a few frames, then shut down. Verify DB contains a match record.	CI job integration.yml passes.
5â€‘Bâ€‘02	Simulate hotâ€‘key presses (via keyboard library or pynput) to toggle overlay, open settings, and open dashboard.	All hotâ€‘keys work in headless environment.
5â€‘Bâ€‘03	Endâ€‘toâ€‘end test of changing a setting (e.g., ghost colour) and confirming the overlay updates (pixel comparison).	Test asserts the pixel colour changed accordingly.
Estimated effort: 6â€¯h

Epicâ€¯5â€‘C â€“ Performance & Stress Tests
Task	Detail	Acceptance
5â€‘Câ€‘01	Write a benchmark script (benchmark_frame_time.py) that runs the frame worker for 500 frames and reports average FPS, max latency, and memory usage (via tracemalloc).	Output shows â‰¥â€¯28â€¯FPS and â‰¤â€¯30â€¯ms latency.
5â€‘Câ€‘02	Add a CI job that runs the benchmark and fails if FPS drops below 25.	CI badge â€œPerformanceâ€¯â‰¥â€¯25â€¯FPSâ€ passes.
5â€‘Câ€‘03	Run memoryâ€‘leak detection (pytest --leak) over 10â€¯k frames.	No incremental memory growth >â€¯1â€¯MiB.
Estimated effort: 4â€¯h

ğŸš¦ Phaseâ€¯6 â€“ Continuousâ€‘Integration & Release Automation
Epicâ€¯6â€‘A â€“ CI Pipeline Overhaul
Task	Detail	Acceptance
6â€‘Aâ€‘01	Create separate GitHub Actions jobs: lint, type-check, unit-tests, ui-tests, performance.	Each job runs on Ubuntuâ€‘latest with a matrix for Pythonâ€¯3.11.
6â€‘Aâ€‘02	Add Xvfb setup for Qt UI tests (apt-get install xvfb).	UI tests run without a physical display.
6â€‘Aâ€‘03	Install ruff (lint) and mypy (type checking) as part of CI.	Badge lint and typeâ€‘checking show â€œpassedâ€.
6â€‘Aâ€‘04	Create a coverage job that uploads the report to Codecov.	Codecov badge added to README.
6â€‘Aâ€‘05	Add caching for pip packages (actions/cache) to speed up CI.	CI time â‰¤â€¯4â€¯minutes.
Estimated effort: 5â€¯h

Epicâ€¯6â€‘B â€“ Release Automation
Task	Detail	Acceptance
6â€‘Bâ€‘01	Configure Semantic Release (semantic-release npm package or release-drafter) to generate a changelog from commit messages.	Merged PR automatically creates a GitHub Release.
6â€‘Bâ€‘02	Add a GitHub Action that builds a standâ€‘alone executable with PyInstaller for Windows/Linux/macOS.	Artifacts available in the Release page.
6â€‘Bâ€‘03	Publish the package to PyPI under a new name (e.g., tetrisâ€‘overlayâ€‘engine). Use twine in a release workflow.	pip install tetris-overlay-engine works.
6â€‘Bâ€‘04	Add preâ€‘release and nightly tags (v2.0â€‘rc, v2.0â€‘nightly).	Developers can install the bleedingâ€‘edge version.
Estimated effort: 4â€¯h

ğŸ“š Phaseâ€¯7 â€“ Documentation, Packaging & Distribution
Epic	Tasks	Acceptance
7â€‘A	Update README.md with a full â€œQuickâ€‘Startâ€ guide, a screenshot gallery, and a â€œFAQâ€.	New README renders correctly on GitHub.
7â€‘B	Add a HOTKEYS.md file (already drafted) and link from README.	Users can find hotâ€‘key list quickly.
7â€‘C	Create a docs/ folder with Sphinx configuration (or MkDocs) that hosts the API reference (ui.settings, stats.db, overlay_renderer).	mkdocs serve works locally; hosted via GitHub Pages.
7â€‘D	Write a contributor guide (CONTRIBUTING.md) covering: coding style, testâ€‘running, PR template, issue labeling.	New contributors can follow the guide.
7â€‘E	Add a CHANGELOG.md that is autoâ€‘generated by Semantic Release.	Release page includes changelog.
7â€‘F	Create a setup.cfg/pyproject.toml that declares entryâ€‘points (tetris_overlay = run_overlay_core:main).	Users can run python -m tetris_overlay.
7â€‘G	Add license file (MIT) and a CODE_OF_CONDUCT.md.	Repository complies with OSS best practices.
Estimated effort: 8â€¯h

âš¡ Phaseâ€¯8 â€“ Performance, Profiling, and Optimization
Epic	Tasks	Acceptance
8â€‘A	Profile the frame loop with cProfile and visualize with snakeviz. Identify hot spots (likely image conversion and AI prediction).	Report frame_time_breakdown.png attached to PR.
8â€‘B	Optimize image conversion (np.array(image) â†’ cv2) by using memoryview and avoiding copies.	Frame latency reduced by â‰¥â€¯5â€¯ms.
8â€‘C	Cache predictionâ€‘agent handles for identical board states (hashâ€‘based memoization).	Duplicate board detection reduces AI call time >â€¯20â€¯%.
8â€‘D	Optionally offâ€‘load the Dellacherie heuristic to a separate process (multiprocessing) and communicate via a simple queue.	Main thread stays â‰¤â€¯20â€¯ms per frame.
8â€‘E	Add GPUâ€‘accelerated ONNX inference (if a GPU is available) â€“ use onnxruntime-gpu.	Inference time drops from ~10â€¯ms to ~2â€¯ms on a CUDA machine.
8â€‘F	Implement dynamic FPS scaling: if frame time exceeds 35â€¯ms, drop to 20â€¯FPS temporarily, otherwise recover to 30â€¯FPS.	No frameâ€‘drops visible to user; CPU stays <â€¯20â€¯%.
8â€‘G	Run a stress test simulating 3 monitors, highâ€‘resolution (4K) captures, and random board sizes.	No crashes, memory consumption <â€¯200â€¯MB.
Estimated effort: 12â€¯h

ğŸ”® Phaseâ€¯9 â€“ Futureâ€‘Proofing & Extensibility
Epic	Tasks	Acceptance
9â€‘A	Define a plugâ€‘in architecture for prediction agents (entry_points in setup.cfg). Allow thirdâ€‘party agents to be installed via pip.	Users can pip install tetrisâ€‘agentâ€‘mycool and select it in Settings.
9â€‘B	Create a configuration schema (config_schema.json) that allows extensions (new visual effects, extra UI panels).	Core validates extra fields gracefully.
9â€‘C	Add Webâ€‘socket server (FastAPI) that streams live board state & predictions to a browser dashboard.	Browser can display realâ€‘time overlay data.
9â€‘D	Implement multiâ€‘player support â€“ handle two separate board captures and display two ghosts simultaneously.	Overlay shows both playersâ€™ ghosts sideâ€‘byâ€‘side.
9â€‘E	Provide sample Dockerfile for running the overlay in a container (use xvfb and pulseaudio for Linux).	docker build -t tetris-overlay . and docker run works.
9â€‘F	Add Internationalisation (i18n) â€“ use gettext for UI strings, provide a template .pot.	UI language can be switched via Settings.
9â€‘G	Create a pluginâ€‘sample repository that demonstrates adding a new visual effect (e.g., â€œparticle trailâ€).	Documentation points to the sample repo.
Estimated effort: 15â€¯h

â±ï¸ Overall Timeline & Milestones
Milestone	Approx. Calendar (working days)	Effort (personâ€‘hours)
M0 â€“ Baseline & Branch	Dayâ€¯1	3â€¯h
M1 â€“ Core Loop Refactor	Daysâ€¯2â€‘4	20â€¯h
M2 â€“ Settings UI	Daysâ€¯5â€‘8	20â€¯h
M3 â€“ Ghost Rendering Engine	Daysâ€¯9â€‘11	12â€¯h
M4 â€“ Stats Collector + Dashboard	Daysâ€¯12â€‘16	25â€¯h
M5 â€“ Test Suite Expansion	Daysâ€¯17â€‘19	18â€¯h
M6 â€“ CI / Release Automation	Daysâ€¯20â€‘22	9â€¯h
M7 â€“ Docs, Packaging, Distribution	Daysâ€¯23â€‘25	8â€¯h
M8 â€“ Performance Profiling	Daysâ€¯26â€‘28	12â€¯h
M9 â€“ Futureâ€‘Proofing (Plugâ€‘ins, Multiâ€‘Player, Web UI)	Daysâ€¯29â€‘35	20â€¯h
M10 â€“ Final Polish & Release	Daysâ€¯36â€‘38	6â€¯h
Total	~38â€¯working days (â‰ˆâ€¯8â€¯weeks)	~153â€¯personâ€‘hours (â‰ˆâ€¯2â€¯fullâ€‘time weeks for a single dev, or 4â€¯weeks for a 2â€‘person team).
Tip: If you want a faster â€œAFKâ€ sprint, split the work across two developers (or two LLM agents) and run phasesâ€¯1â€‘4 in parallel, then converge for testing and CI.

âš ï¸ Risks & Mitigations
Risk	Impact	Mitigation
Circular imports (Settings â†” Renderer)	Crash on startâ€‘up	Use local imports inside methods, keep CURRENT as a separate module, avoid topâ€‘level crossâ€‘references.
Qtâ€‘related CI failures (no display)	UI tests fail, pipeline red	Use xvfb in GitHub Actions, set DISPLAY=:99.
SQLite locking when many frames write simultaneously	Data loss / crashes	Use shortâ€‘lived sessions (with get_session()) and enable WAL mode (PRAGMA journal_mode=WAL).
Performance regression after adding UI layers	FPS drops below 25	Keep profiling checkpoints after each major change, enforce performance CI gate.
Hotâ€‘key conflicts on some keyboards (e.g., Ctrl+Alt+S collides with OS)	Users cannot open dashboard	Provide a â€œreset to default hotâ€‘keysâ€ button; allow editing all keys in Settings UI.
Dependency drift (new versions of PySide/Pygame break imports)	Build breaks for contributors	Pin critical versions in requirements.txt with >=/< ranges, run a weekly Dependabot check.
Large binary assets (screenshots, calibration data) accidentally committed	Repo bloat	Add .gitignore entry for *.png, *.jpg, and *.bmp in config/ and captured/. Use Git LFS if needed.
ğŸ¤–â€¯How to Hand This to an LLM / Automation Script
Create a â€œtask queueâ€ JSON that lists each subâ€‘task with:

name (e.g., phase1â€‘rendererâ€‘singleton)
file_path (relative path)
action (add, replace, append)
patch (the code block from the tables above)
test_command (optional, e.g., pytest tests/test_settings.py)
expected_exit_code (0).
Loop through the queue:

git checkout dev/fullâ€‘refactor
Apply the patch (apply_patch(patch)).
Run the associated test command.
If the test fails, run git reset --hard HEAD and retry with a â€œfallbackâ€ patch (e.g., using a simpler implementation).
On success, commit with the provided commit message.
Parallelise: run separate workers for independent phases (e.g., UI vs. core). Use a small dependency graph so phases that need the renderer (Phaseâ€¯3) wait for Phaseâ€¯1 to finish.

CI Integration: after the LLM finishes the branch, push it and let GitHub Actions run the full pipeline automatically.

A simple pseudoâ€‘script (Python) could look like:

import json, subprocess, pathlib, sys

tasks = json.load(open("taskâ€‘queue.json"))
repo_root = pathlib.Path(__file__).parent

def run_cmd(cmd):
    return subprocess.run(cmd, shell=True, cwd=repo_root)

for t in tasks:
    print(f"â©  Working on {t['name']}")
    # 1. apply patch
    patch_path = repo_root / t["file_path"]
    patch_content = t["patch"]
    patch_path.write_text(patch_content)   # simple replace for demo
    # 2. run test (if any)
    if t.get("test_command"):
        result = run_cmd(t["test_command"])
        if result.returncode != t.get("expected_exit_code", 0):
            print(f"âŒ  Test failed for {t['name']}, aborting.")
            sys.exit(1)
    # 3. commit
    run_cmd(f'git add {t["file_path"]}')
    run_cmd(f'git commit -m "{t["commit_msg"]}"')
print("âœ…  Everything applied!")
You can enrich this script with apply_patch that uses git apply for more complex diff handling, but the concept stays the same.

ğŸ‰â€¯Final Remarks
The plan is intentionally exhaustive â€“ you can pick and choose the phases you want to work on now, and defer others to later sprints.
Each phase produces a clear deliverable (code, tests, docs, CI job).
By the end of Phaseâ€¯9 youâ€™ll have a battleâ€‘tested, wellâ€‘documented, extensible, productionâ€‘grade overlay that anyone can fork, improve, or ship as a binary.
Feel free to cherryâ€‘pick the tasks that give you the most immediate value (e.g., getting the 30â€¯FPS loop stable) and then let the LLM continue to hammer out the UI, stats, and CI work while you â€œcookâ€.

Good luck, and enjoy the automation! ğŸš€